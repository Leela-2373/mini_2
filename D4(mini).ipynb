{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021514,
     "end_time": "2021-01-05T09:13:29.257294",
     "exception": false,
     "start_time": "2021-01-05T09:13:29.235780",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ML Driven Anomaly Detection  for IoT Edge Devices: Insights from ADMM-Based Frequency Management\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-05T09:13:29.349482Z",
     "iopub.status.busy": "2021-01-05T09:13:29.348501Z",
     "iopub.status.idle": "2021-01-05T09:13:29.352715Z",
     "shell.execute_reply": "2021-01-05T09:13:29.351769Z"
    },
    "papermill": {
     "duration": 0.033834,
     "end_time": "2021-01-05T09:13:29.352867",
     "exception": false,
     "start_time": "2021-01-05T09:13:29.319033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.029346,
     "end_time": "2021-01-05T09:13:29.412442",
     "exception": false,
     "start_time": "2021-01-05T09:13:29.383096",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2021-01-05T09:13:29.485562Z",
     "iopub.status.busy": "2021-01-05T09:13:29.484670Z",
     "iopub.status.idle": "2021-01-05T09:13:39.281105Z",
     "shell.execute_reply": "2021-01-05T09:13:39.282223Z"
    },
    "papermill": {
     "duration": 9.839763,
     "end_time": "2021-01-05T09:13:39.282387",
     "exception": false,
     "start_time": "2021-01-05T09:13:29.442624",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(r\"dataset.csv\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['anomaly'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Dependent and independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-05T09:13:40.525770Z",
     "iopub.status.busy": "2021-01-05T09:13:40.524926Z",
     "iopub.status.idle": "2021-01-05T09:13:40.530894Z",
     "shell.execute_reply": "2021-01-05T09:13:40.530199Z"
    },
    "papermill": {
     "duration": 0.039888,
     "end_time": "2021-01-05T09:13:40.531005",
     "exception": false,
     "start_time": "2021-01-05T09:13:40.491117",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X= dataset.iloc[:,0:12]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y= dataset.iloc[:, -1]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a count plot\n",
    "sns.set(style=\"darkgrid\")  # Set the style of the plot\n",
    "plt.figure(figsize=(8, 6))  # Set the figure size\n",
    "# Replace 'dataset' with your actual DataFrame and 'Drug' with the column name\n",
    "ax = sns.countplot(x=y,palette=\"Set3\")\n",
    "plt.title(\"Count Plot\")  # Add a title to the plot\n",
    "plt.xlabel(\"Categories\")  # Add label to x-axis\n",
    "plt.ylabel(\"Count\")  # Add label to y-axis\n",
    "# Annotate each bar with its count value\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', fontsize=10, color='black', xytext=(0, 5),\n",
    "                textcoords='offset points')\n",
    "\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"Frequency Drift\",\"Capacity Breach\",\"Dual Signal Interference\",\"Request Overload\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining global variables to store accuracy and other metrics\n",
    "precision = []\n",
    "recall = []\n",
    "fscore = []\n",
    "accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate various metrics such as accuracy, precision etc\n",
    "def calculateMetrics(algorithm, predict, testY):\n",
    "    testY = testY.astype('int')\n",
    "    predict = predict.astype('int')\n",
    "    p = precision_score(testY, predict,average='macro') * 100\n",
    "    r = recall_score(testY, predict,average='macro') * 100\n",
    "    f = f1_score(testY, predict,average='macro') * 100\n",
    "    a = accuracy_score(testY,predict)*100 \n",
    "    accuracy.append(a)\n",
    "    precision.append(p)\n",
    "    recall.append(r)\n",
    "    fscore.append(f)\n",
    "    print(algorithm+' Accuracy    : '+str(a))\n",
    "    print(algorithm+' Precision   : '+str(p))\n",
    "    print(algorithm+' Recall      : '+str(r))\n",
    "    print(algorithm+' FSCORE      : '+str(f))\n",
    "    report=classification_report(predict, testY,target_names=labels)\n",
    "    print('\\n',algorithm+\" classification report\\n\",report)\n",
    "    conf_matrix = confusion_matrix(testY, predict) \n",
    "    plt.figure(figsize =(5, 5)) \n",
    "    ax = sns.heatmap(conf_matrix, xticklabels = labels, yticklabels = labels, annot = True, cmap=\"Blues\" ,fmt =\"g\");\n",
    "    ax.set_ylim([0,len(labels)])\n",
    "    plt.title(algorithm+\" Confusion matrix\") \n",
    "    plt.ylabel('True class') \n",
    "    plt.xlabel('Predicted class') \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regresssion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the pkl file exists\n",
    "if os.path.exists('LogisticRegression_weights.pkl'):\n",
    "    # Load the model from the pkl file\n",
    "    rf_classifier= joblib.load('LogisticRegression_weights.pkl')\n",
    "    predict = rf_classifier.predict(X_test)\n",
    "    calculateMetrics(\"LogisticRegression\", predict, y_test)\n",
    "else:\n",
    "    clf = LogisticRegression()\n",
    "    # Train the classifier on the training data\n",
    "    clf.fit(X_train, y_train)\n",
    "    # Make predictions on the test data\n",
    "    predict=clf.predict(X_test)\n",
    "    joblib.dump(clf, 'LogisticRegression_weights.pkl')\n",
    "    print(\"LogisticRegression model trained and model weights saved.\")\n",
    "    calculateMetrics(\"LogisticRegression\", predict, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decisiontree with AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the pkl file exists\n",
    "if os.path.exists('ada_weights.pkl'):\n",
    "    # Load the model from the pkl file\n",
    "    classifier= joblib.load('ada_weights.pkl')\n",
    "    predict = classifier.predict(X_test)\n",
    "    calculateMetrics(\"DTC with AdaBoost Classifier\", predict, y_test)\n",
    "else:\n",
    "    # Initialize a DecisionTreeClassifier as the base estimator for AdaBoost\n",
    "    base_estimator = DecisionTreeClassifier(max_depth=10)\n",
    "\n",
    "    # Initialize the AdaBoost model with chosen parameters\n",
    "    classifier= AdaBoostClassifier(base_estimator=base_estimator)\n",
    "    \n",
    "    # Train the classifier on the training data\n",
    "    classifier.fit(X_train, y_train)\n",
    "    # Make predictions on the test data\n",
    "    predict=classifier.predict(X_test)\n",
    "    # Save the model weights to a pkl file\n",
    "    joblib.dump(classifier, 'ada_weights.pkl')\n",
    "    print(\"DT with Adaboost classifier_model trained and model weights saved.\")\n",
    "    calculateMetrics(\"DTC with AdaBoost Classifier\", predict, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performance Comparision of both the algorithmns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#showing all algorithms performance values\n",
    "columns = [\"Algorithm Name\",\"Precison\",\"Recall\",\"FScore\",\"Accuracy\"]\n",
    "values = []\n",
    "algorithm_names = [\"LogisticRegression\",\"DTC with AdaBoost Classifier\"]\n",
    "for i in range(len(algorithm_names)):\n",
    "    values.append([algorithm_names[i],precision[i],recall[i],fscore[i],accuracy[i]])\n",
    "    \n",
    "temp = pd.DataFrame(values,columns=columns)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction onnew test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv(\"test.csv\")\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the selected test data\n",
    "predict = classifier.predict(test)\n",
    "\n",
    "# Loop through each prediction and print the corresponding row\n",
    "for i, p in enumerate(predict):\n",
    "    print(test.iloc[i])  # Print the row\n",
    "    print(f\"Row {i}:************************************************** {labels[p]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Predicted'] = [labels[p] for p in predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "papermill": {
   "duration": 623.697222,
   "end_time": "2021-01-05T09:23:46.059736",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-01-05T09:13:22.362514",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
